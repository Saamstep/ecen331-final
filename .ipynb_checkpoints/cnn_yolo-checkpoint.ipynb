{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6257c5e9-54d9-4692-aec0-cc0235b63a87",
   "metadata": {},
   "source": [
    "# OpenCV for real time video object detection\n",
    "## Applications: road obstacle avoidance, pedestrians, or traffic light detection/classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb210ee-e84c-4003-8b18-167fcf1b5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "# import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dcc5fa-977c-4f9a-9d8a-24b8342cfee8",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "* https://public.roboflow.com/object-detection/pothole\n",
    "* https://public.roboflow.com/object-detection/self-driving-car\n",
    "\n",
    "## Testing Data\n",
    "https://www.kaggle.com/datasets/robikscube/driving-video-with-object-tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb628c-1850-48d3-87c6-9ca9728a42ce",
   "metadata": {},
   "source": [
    "## Model Prep/Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77461aa-757a-4169-aded-9c0663e9a6bc",
   "metadata": {},
   "source": [
    "### Prep Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d911813-a055-4249-bc4f-ff0e91ef73aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Need to manually split Self Driving Car Dataset\n",
    "# import os\n",
    "# import shutil\n",
    "# import random\n",
    "\n",
    "# # Set the path to your \"export\" folder\n",
    "# export_path = \"./export\"\n",
    "\n",
    "# # Set the path to the new folders (train, val, test)\n",
    "# train_path = \"./train\"\n",
    "# val_path = \"./valid\"\n",
    "# test_path = \"./test\"\n",
    "\n",
    "# # Set the split ratios (adjust as needed)\n",
    "# train_ratio = 0.8  # 80% for training\n",
    "# val_ratio = 0.1    # 10% for validation\n",
    "# test_ratio = 0.1   # 10% for testing\n",
    "\n",
    "# # Create the new folders if they don't exist\n",
    "# os.makedirs(os.path.join(train_path, \"images\"), exist_ok=True)\n",
    "# os.makedirs(os.path.join(train_path, \"labels\"), exist_ok=True)\n",
    "# os.makedirs(os.path.join(val_path, \"images\"), exist_ok=True)\n",
    "# os.makedirs(os.path.join(val_path, \"labels\"), exist_ok=True)\n",
    "# os.makedirs(os.path.join(test_path, \"images\"), exist_ok=True)\n",
    "# os.makedirs(os.path.join(test_path, \"labels\"), exist_ok=True)\n",
    "# # os.makedirs(val_path, exist_ok=True)\n",
    "# # os.makedirs(test_path, exist_ok=True)\n",
    "\n",
    "# # Get the list of image files in the \"images\" folder\n",
    "# image_folder_path = os.path.join(export_path, \"images\")\n",
    "# image_files = [f for f in os.listdir(image_folder_path) if f.endswith(('.jpg', '.png'))]\n",
    "\n",
    "# # Randomly shuffle the list of image files\n",
    "# random.shuffle(image_files)\n",
    "\n",
    "# # Calculate the number of images for each split\n",
    "# num_images = len(image_files)\n",
    "# num_train = int(train_ratio * num_images)\n",
    "# num_val = int(val_ratio * num_images)\n",
    "# num_test = int(test_ratio * num_images)\n",
    "\n",
    "# # Split the image files\n",
    "# train_images = image_files[:num_train]\n",
    "# val_images = image_files[num_train:num_train + num_val]\n",
    "# test_images = image_files[num_train + num_val:]\n",
    "\n",
    "# # Move the images to their respective folders\n",
    "# for img in train_images:\n",
    "#     shutil.move(os.path.join(image_folder_path, img), os.path.join(os.path.join(train_path, \"images\"), img))\n",
    "\n",
    "# for img in val_images:\n",
    "#     shutil.move(os.path.join(image_folder_path, img), os.path.join(os.path.join(val_path, \"images\"), img))\n",
    "\n",
    "# for img in test_images:\n",
    "#     shutil.move(os.path.join(image_folder_path, img), os.path.join(os.path.join(test_path, \"images\"), img))\n",
    "\n",
    "# # Repeat the same process for the \"labels\" folder\n",
    "\n",
    "# label_folder_path = os.path.join(export_path, \"labels\")\n",
    "\n",
    "# # Move the label files to their respective folders\n",
    "# for lbl in train_images:\n",
    "#     lbl = lbl.replace('.jpg', '.txt')\n",
    "#     shutil.move(os.path.join(label_folder_path, lbl), os.path.join(os.path.join(train_path, \"labels\"), lbl))\n",
    "\n",
    "# for lbl in val_images:\n",
    "#     lbl = lbl.replace('.jpg', '.txt')\n",
    "#     shutil.move(os.path.join(label_folder_path, lbl), os.path.join(os.path.join(val_path, \"labels\"), lbl))\n",
    "\n",
    "# for lbl in test_images:\n",
    "#     lbl = lbl.replace('.jpg', '.txt')\n",
    "#     shutil.move(os.path.join(label_folder_path, lbl), os.path.join(os.path.join(test_path, \"labels\"), lbl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "355a2386-44fe-4b2b-86cf-a5c195ca695d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory containing your annotation files\n",
    "annotations_dir = 'ConeTest.v1i.yolov8/train/labels'  # Replace with your actual directory path\n",
    "\n",
    "# The class ID for pothole that needs to be changed from 0 to 11\n",
    "old_class_id = 0\n",
    "new_class_id = 12\n",
    "\n",
    "# Iterate over all annotation files\n",
    "for filename in os.listdir(annotations_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(annotations_dir, filename)\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        \n",
    "        updated_lines = []\n",
    "        \n",
    "        # Iterate over each line (annotation) and check the class index\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])  # Get the class ID\n",
    "            \n",
    "            # If the class ID is the old class (pothole), change it to the new class ID\n",
    "            if class_id == old_class_id:\n",
    "                parts[0] = str(new_class_id)  # Update class ID\n",
    "            \n",
    "            updated_lines.append(' '.join(parts) + '\\n')\n",
    "        \n",
    "        # Write the updated lines back to the file\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.writelines(updated_lines)\n",
    "\n",
    "# print(\"Class ID for potholes has been updated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b36528-b41b-441d-9110-f9c37860a0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.91 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.81  Python-3.10.16 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 8192MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=custom.yaml, epochs=3, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train9, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train9\n",
      "Overriding model.yaml nc=80 with nc=13\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753847  ultralytics.nn.modules.head.Detect           [13, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,013,383 parameters, 3,013,367 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\samue\\Documents\\projects\\ecen331\\final\\train\\labels... 2214 images, 266 backgrounds, 0 corrup\u001b[0m"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "results = model.train(\n",
    "    data='custom.yaml',\n",
    "    epochs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b002021-980c-49ec-82d8-8a7842a59623",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.val()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710bd173-b7a0-40b3-b06d-dd193e4ef8b3",
   "metadata": {},
   "source": [
    "## OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719f58ba-f8d9-4e6b-a3b8-51c6b5d7e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV Setup\n",
    "cap = cv.VideoCapture('./archive/train/0000f77c-6257be58.mov')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "        height, width = frame.shape[:2]\n",
    "\n",
    "        blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "\n",
    "        net.setInput(blob)\n",
    "\n",
    "        outs = net.forward(output_layers)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
