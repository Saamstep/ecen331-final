{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6257c5e9-54d9-4692-aec0-cc0235b63a87",
   "metadata": {},
   "source": [
    "# OpenCV for real time video object detection\n",
    "## Applications: road obstacle avoidance, pedestrians, or traffic light detection/classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cb210ee-e84c-4003-8b18-167fcf1b5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dcc5fa-977c-4f9a-9d8a-24b8342cfee8",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "* https://public.roboflow.com/object-detection/pothole\n",
    "* https://public.roboflow.com/object-detection/self-driving-car\n",
    "\n",
    "## Testing Data\n",
    "https://www.kaggle.com/datasets/robikscube/driving-video-with-object-tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb628c-1850-48d3-87c6-9ca9728a42ce",
   "metadata": {},
   "source": [
    "## Model Prep/Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77461aa-757a-4169-aded-9c0663e9a6bc",
   "metadata": {},
   "source": [
    "### Prep Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d911813-a055-4249-bc4f-ff0e91ef73aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Need to manually split Self Driving Car Dataset\n",
    "# import os\n",
    "# import shutil\n",
    "# import random\n",
    "\n",
    "# # Set the path to your \"export\" folder\n",
    "# export_path = \"./export\"\n",
    "\n",
    "# # Set the path to the new folders (train, val, test)\n",
    "# train_path = \"./train\"\n",
    "# val_path = \"./valid\"\n",
    "# test_path = \"./test\"\n",
    "\n",
    "# # Set the split ratios (adjust as needed)\n",
    "# train_ratio = 0.8  # 80% for training\n",
    "# val_ratio = 0.1    # 10% for validation\n",
    "# test_ratio = 0.1   # 10% for testing\n",
    "\n",
    "# # Create the new folders if they don't exist\n",
    "# os.makedirs(os.path.join(train_path, \"images\"), exist_ok=True)\n",
    "# os.makedirs(os.path.join(train_path, \"labels\"), exist_ok=True)\n",
    "# os.makedirs(os.path.join(val_path, \"images\"), exist_ok=True)\n",
    "# os.makedirs(os.path.join(val_path, \"labels\"), exist_ok=True)\n",
    "# os.makedirs(os.path.join(test_path, \"images\"), exist_ok=True)\n",
    "# os.makedirs(os.path.join(test_path, \"labels\"), exist_ok=True)\n",
    "# # os.makedirs(val_path, exist_ok=True)\n",
    "# # os.makedirs(test_path, exist_ok=True)\n",
    "\n",
    "# # Get the list of image files in the \"images\" folder\n",
    "# image_folder_path = os.path.join(export_path, \"images\")\n",
    "# image_files = [f for f in os.listdir(image_folder_path) if f.endswith(('.jpg', '.png'))]\n",
    "\n",
    "# # Randomly shuffle the list of image files\n",
    "# random.shuffle(image_files)\n",
    "\n",
    "# # Calculate the number of images for each split\n",
    "# num_images = len(image_files)\n",
    "# num_train = int(train_ratio * num_images)\n",
    "# num_val = int(val_ratio * num_images)\n",
    "# num_test = int(test_ratio * num_images)\n",
    "\n",
    "# # Split the image files\n",
    "# train_images = image_files[:num_train]\n",
    "# val_images = image_files[num_train:num_train + num_val]\n",
    "# test_images = image_files[num_train + num_val:]\n",
    "\n",
    "# # Move the images to their respective folders\n",
    "# for img in train_images:\n",
    "#     shutil.move(os.path.join(image_folder_path, img), os.path.join(os.path.join(train_path, \"images\"), img))\n",
    "\n",
    "# for img in val_images:\n",
    "#     shutil.move(os.path.join(image_folder_path, img), os.path.join(os.path.join(val_path, \"images\"), img))\n",
    "\n",
    "# for img in test_images:\n",
    "#     shutil.move(os.path.join(image_folder_path, img), os.path.join(os.path.join(test_path, \"images\"), img))\n",
    "\n",
    "# # Repeat the same process for the \"labels\" folder\n",
    "\n",
    "# label_folder_path = os.path.join(export_path, \"labels\")\n",
    "\n",
    "# # Move the label files to their respective folders\n",
    "# for lbl in train_images:\n",
    "#     lbl = lbl.replace('.jpg', '.txt')\n",
    "#     shutil.move(os.path.join(label_folder_path, lbl), os.path.join(os.path.join(train_path, \"labels\"), lbl))\n",
    "\n",
    "# for lbl in val_images:\n",
    "#     lbl = lbl.replace('.jpg', '.txt')\n",
    "#     shutil.move(os.path.join(label_folder_path, lbl), os.path.join(os.path.join(val_path, \"labels\"), lbl))\n",
    "\n",
    "# for lbl in test_images:\n",
    "#     lbl = lbl.replace('.jpg', '.txt')\n",
    "#     shutil.move(os.path.join(label_folder_path, lbl), os.path.join(os.path.join(test_path, \"labels\"), lbl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "355a2386-44fe-4b2b-86cf-a5c195ca695d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory containing your annotation files\n",
    "annotations_dir = 'ConeTest.v1i.yolov8/train/labels'  # Replace with your actual directory path\n",
    "\n",
    "# The class ID for pothole that needs to be changed from 0 to 11\n",
    "old_class_id = 0\n",
    "new_class_id = 12\n",
    "\n",
    "# Iterate over all annotation files\n",
    "for filename in os.listdir(annotations_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(annotations_dir, filename)\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        \n",
    "        updated_lines = []\n",
    "        \n",
    "        # Iterate over each line (annotation) and check the class index\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])  # Get the class ID\n",
    "            \n",
    "            # If the class ID is the old class (pothole), change it to the new class ID\n",
    "            if class_id == old_class_id:\n",
    "                parts[0] = str(new_class_id)  # Update class ID\n",
    "            \n",
    "            updated_lines.append(' '.join(parts) + '\\n')\n",
    "        \n",
    "        # Write the updated lines back to the file\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.writelines(updated_lines)\n",
    "\n",
    "# print(\"Class ID for potholes has been updated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26b36528-b41b-441d-9110-f9c37860a0c2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.91 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.81  Python-3.10.16 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 8192MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=custom.yaml, epochs=30, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train11, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train11\n",
      "Overriding model.yaml nc=80 with nc=13\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753847  ultralytics.nn.modules.head.Detect           [13, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,013,383 parameters, 3,013,367 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\samue\\Documents\\projects\\ecen331\\final\\train\\labels.cache... 14521 images, 788 backgrounds, 0\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\samue\\Documents\\projects\\ecen331\\final\\valid\\labels.cache... 2785 images, 96 backgrounds, 0 cor\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train11\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000588, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train11\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/30      2.87G      1.689      1.905      1.159        106        640: 100%|██████████| 908/908 [01:54<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.608      0.299      0.274      0.142\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/30         3G      1.578      1.243      1.124        131        640: 100%|██████████| 908/908 [01:46<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.633      0.326      0.338      0.174\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/30      2.86G      1.553       1.12      1.119        166        640: 100%|██████████| 908/908 [01:42<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.666      0.319      0.367      0.191\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/30       2.5G      1.529      1.055      1.113        127        640: 100%|██████████| 908/908 [01:41<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.704      0.351      0.399      0.204\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/30      2.71G      1.502      1.008      1.099        134        640: 100%|██████████| 908/908 [01:42<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572       0.67      0.378      0.424      0.231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/30      2.98G      1.479     0.9757      1.091        109        640: 100%|██████████| 908/908 [01:42<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.711      0.391      0.428      0.224\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/30      2.96G      1.462     0.9503      1.084        125        640: 100%|██████████| 908/908 [01:42<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.732      0.396      0.464      0.247\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/30      2.97G       1.45     0.9297      1.079        123        640: 100%|██████████| 908/908 [01:42<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.802      0.403      0.469      0.251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/30      3.25G      1.433     0.9124      1.073        135        640: 100%|██████████| 908/908 [01:43<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.766      0.419      0.478      0.261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/30      2.83G      1.426     0.8994       1.07         88        640: 100%|██████████| 908/908 [01:44<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.739      0.413      0.502      0.275\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/30      2.97G      1.413     0.8841      1.066        107        640: 100%|██████████| 908/908 [01:42<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.829      0.431      0.521       0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/30      2.44G      1.408     0.8708      1.062         92        640: 100%|██████████| 908/908 [01:42<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.835      0.433       0.52      0.283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/30      3.15G      1.399     0.8639      1.057         65        640: 100%|██████████| 908/908 [01:42<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.836       0.44      0.556      0.307\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/30      2.88G      1.393     0.8583      1.055        156        640: 100%|██████████| 908/908 [01:42<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.848      0.451      0.561      0.309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/30      2.58G      1.384     0.8437      1.052         54        640: 100%|██████████| 908/908 [01:42<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.838      0.463       0.57      0.313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/30      2.71G      1.368     0.8288      1.044        173        640: 100%|██████████| 908/908 [01:43<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.704      0.539      0.583      0.326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/30      2.62G      1.366     0.8217      1.044         85        640: 100%|██████████| 908/908 [01:42<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.756      0.537      0.595      0.328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/30      3.05G      1.364     0.8206       1.04        203        640: 100%|██████████| 908/908 [01:41<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.842      0.485       0.59      0.325\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/30      2.72G      1.352     0.8085      1.036        154        640: 100%|██████████| 908/908 [01:42<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.753      0.542      0.608      0.335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/30      2.77G      1.345     0.8017      1.034         74        640: 100%|██████████| 908/908 [01:43<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.865      0.487      0.604      0.334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/30      2.23G      1.349     0.7804      1.046         88        640: 100%|██████████| 908/908 [01:41<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.792      0.546      0.607      0.339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/30      2.38G      1.337     0.7698      1.041         67        640: 100%|██████████| 908/908 [01:40<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.835      0.544      0.626       0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/30      2.37G      1.324     0.7557      1.038         48        640: 100%|██████████| 908/908 [01:40<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.855       0.54      0.636      0.351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/30      2.32G      1.318     0.7485      1.035         58        640: 100%|██████████| 908/908 [01:39<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.826      0.555      0.633      0.357\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/30      2.46G      1.307     0.7374       1.03         76        640: 100%|██████████| 908/908 [01:39<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.858      0.552      0.647      0.364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/30      2.34G      1.305     0.7326      1.028         85        640: 100%|██████████| 908/908 [01:40<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.863       0.54      0.648       0.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/30      2.44G      1.291     0.7199      1.023         70        640: 100%|██████████| 908/908 [01:41<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.861       0.55      0.648       0.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/30       2.4G      1.287     0.7174      1.021         76        640: 100%|██████████| 908/908 [01:40<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.828      0.581       0.66      0.377\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/30      2.44G      1.278     0.7092      1.018         97        640: 100%|██████████| 908/908 [01:39<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.831      0.583      0.665      0.377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/30       2.2G      1.273     0.7025      1.017         93        640: 100%|██████████| 908/908 [01:39<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.835      0.586      0.667      0.381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "30 epochs completed in 0.981 hours.\n",
      "Optimizer stripped from runs\\detect\\train11\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train11\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train11\\weights\\best.pt...\n",
      "Ultralytics 8.3.81  Python-3.10.16 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 8192MiB)\n",
      "Model summary (fused): 72 layers, 3,008,183 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.819      0.582      0.666       0.38\n",
      "                 biker         44         70      0.732      0.529      0.594       0.33\n",
      "                   car        963       4783      0.814      0.816      0.858      0.585\n",
      "            pedestrian        319       1055      0.691      0.595      0.659      0.335\n",
      "          trafficLight         96        159      0.825      0.836      0.886       0.51\n",
      "    trafficLight-Green        145        389      0.809      0.674      0.738      0.349\n",
      "trafficLight-GreenLeft         14         23          1     0.0827      0.226     0.0986\n",
      "      trafficLight-Red        155        377      0.873      0.743      0.809      0.461\n",
      "  trafficLight-RedLeft         38         50      0.771       0.58      0.669      0.339\n",
      "   trafficLight-Yellow          7         12      0.869        0.5       0.79      0.435\n",
      "trafficLight-YellowLeft          2          2          1          0     0.0622     0.0108\n",
      "                 truck        201        249      0.828      0.803      0.855      0.605\n",
      "               pothole        133        330      0.587      0.669      0.688      0.403\n",
      "                  cone       1571      17073      0.848      0.744      0.828      0.485\n",
      "Speed: 0.2ms preprocess, 1.1ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train11\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "results = model.train(\n",
    "    data='custom.yaml',\n",
    "    epochs=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b002021-980c-49ec-82d8-8a7842a59623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.81  Python-3.10.16 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 8192MiB)\n",
      "Model summary (fused): 72 layers, 3,008,183 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\samue\\Documents\\projects\\ecen331\\final\\valid\\labels.cache... 2785 images, 96 backgrounds, 0 cor\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 175/175 [00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2785      24572      0.837      0.587      0.667      0.382\n",
      "                 biker         44         70      0.765      0.529      0.599      0.333\n",
      "                   car        963       4783      0.818      0.814      0.858      0.585\n",
      "            pedestrian        319       1055      0.694      0.587      0.659      0.337\n",
      "          trafficLight         96        159      0.832      0.836      0.883       0.51\n",
      "    trafficLight-Green        145        389       0.81      0.668      0.738      0.351\n",
      "trafficLight-GreenLeft         14         23          1     0.0786      0.226     0.0977\n",
      "      trafficLight-Red        155        377      0.874      0.745      0.812      0.467\n",
      "  trafficLight-RedLeft         38         50      0.802       0.58      0.669      0.339\n",
      "   trafficLight-Yellow          7         12          1      0.575      0.799      0.441\n",
      "trafficLight-YellowLeft          2          2          1          0     0.0603     0.0106\n",
      "                 truck        201        249       0.83      0.803      0.855      0.603\n",
      "               pothole        133        330      0.599      0.673      0.689      0.405\n",
      "                  cone       1571      17073      0.852      0.743       0.83      0.487\n",
      "Speed: 0.2ms preprocess, 1.7ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train112\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "metrics = model.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41e0a02c-dfc5-4396-8f14-27677cd3d60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.81  Python-3.10.16 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 8192MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\samue\\Documents\\projects\\ecen331\\final\\test\\labels... 1924 images, 104 backgrounds, 0 corrupt: \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\samue\\Documents\\projects\\ecen331\\final\\test\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 121/121 [00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1924      15974      0.829      0.554      0.663      0.387\n",
      "                 biker         52         90      0.757      0.378      0.464      0.246\n",
      "                   car        954       4647       0.88      0.781      0.851       0.58\n",
      "            pedestrian        326       1088       0.79      0.519      0.648      0.314\n",
      "          trafficLight        112        170      0.852      0.844       0.88      0.551\n",
      "    trafficLight-Green        172        389      0.846      0.652      0.736      0.343\n",
      "trafficLight-GreenLeft         18         28          1          0      0.168       0.07\n",
      "      trafficLight-Red        147        346      0.897      0.755      0.822      0.495\n",
      "  trafficLight-RedLeft         42         58      0.847      0.397      0.532      0.294\n",
      "   trafficLight-Yellow          3          5      0.684        0.2      0.479      0.245\n",
      "                 truck        202        247      0.854      0.755      0.857      0.591\n",
      "               pothole         67        154      0.647      0.675      0.694      0.432\n",
      "                  cone        782       8752      0.895      0.697      0.825      0.486\n",
      "Speed: 0.2ms preprocess, 1.9ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train113\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "eval = model.val(split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710bd173-b7a0-40b3-b06d-dd193e4ef8b3",
   "metadata": {},
   "source": [
    "## OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "719f58ba-f8d9-4e6b-a3b8-51c6b5d7e723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Model in Path:  C:\\Users\\samue\\Documents\\projects\\ecen331\\final\\runs\\detect\\train11\\weights\\best.pt\n"
     ]
    }
   ],
   "source": [
    "# OpenCV Setup\n",
    "trainIteration = '11'\n",
    "modelPath = os.getcwd() + '\\\\runs\\\\detect\\\\train' + trainIteration + '\\\\weights\\\\best.pt'\n",
    "print(\"Using Model in Path: \", modelPath)\n",
    "\n",
    "model = YOLO(modelPath)\n",
    "# model.export(format=\"onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "718677e7-662a-4580-847f-72c766db2d8e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "trainIteration = '11'\n",
    "\n",
    "onnx_model = os.getcwd() + '\\\\runs\\\\detect\\\\train' + trainIteration + '\\\\weights\\\\best.onnx'\n",
    "\n",
    "net = cv.dnn.readNetFromONNX(onnx_model)\n",
    "\n",
    "net.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv.dnn.DNN_TARGET_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b7a40-3651-4c59-82f5-37b9312de28c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cap = cv.VideoCapture('./archive/train/0000f77c-6257be58.mov')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    # Blob creation for object detection\n",
    "    blob = cv.dnn.blobFromImage(frame, 1/255.0, (640, 640), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "\n",
    "    outs = net.forward()\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                \n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "                print(class_id)\n",
    "\n",
    "    # Non-Maximum Suppression\n",
    "    indices = cv.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    \n",
    "    # Corrected: Check if indices are not empty\n",
    "    if len(indices) > 0:  # Check if indices is not empty\n",
    "        for i in indices.flatten():\n",
    "            box = boxes[i]\n",
    "            x, y, w, h = box[0], box[1], box[2], box[3]\n",
    "            label = classes[class_ids[i]]\n",
    "            confidence = round(confidences[i], 2)\n",
    "            cv.rectangle(frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "            cv.putText(frame, f\"{label} {confidence}\", (x, y + 20), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv.imshow('Object Detection', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture object and destroy all OpenCV windows\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4095c7b2-cecb-4760-b9a0-230b20c4d865",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124marchive\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m0000f77c-6257be58.mov\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cap = cv.VideoCapture('\\\\archive\\\\train\\\\0000f77c-6257be58.mov')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "    results = model.track(frame, stream=True)\n",
    "\n",
    "    for result in results:\n",
    "        class_names = result.names\n",
    "\n",
    "        cv.imshow('frame', frame)\n",
    "        if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b141e17-6758-4084-8855-cd71616091a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the model\n",
    "yolo = YOLO('yolov8s.pt')\n",
    "\n",
    "# Load the video capture\n",
    "videoCap = cv2.VideoCapture(0)\n",
    "\n",
    "# Function to get class colors\n",
    "def getColours(cls_num):\n",
    "    base_colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]\n",
    "    color_index = cls_num % len(base_colors)\n",
    "    increments = [(1, -2, 1), (-2, 1, -1), (1, -1, 2)]\n",
    "    color = [base_colors[color_index][i] + increments[color_index][i] * \n",
    "    (cls_num // len(base_colors)) % 256 for i in range(3)]\n",
    "    return tuple(color)\n",
    "\n",
    "while True:\n",
    "    ret, frame = videoCap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "    results = yolo.track(frame, stream=True)\n",
    "\n",
    "\n",
    "    for result in results:\n",
    "        # get the classes names\n",
    "        classes_names = result.names\n",
    "\n",
    "        # iterate over each box\n",
    "        for box in result.boxes:\n",
    "            # check if confidence is greater than 40 percent\n",
    "            if box.conf[0] > 0.4:\n",
    "                # get coordinates\n",
    "                [x1, y1, x2, y2] = box.xyxy[0]\n",
    "                # convert to int\n",
    "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "                # get the class\n",
    "                cls = int(box.cls[0])\n",
    "\n",
    "                # get the class name\n",
    "                class_name = classes_names[cls]\n",
    "\n",
    "                # get the respective colour\n",
    "                colour = getColours(cls)\n",
    "\n",
    "                # draw the rectangle\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), colour, 2)\n",
    "\n",
    "                # put the class name and confidence on the image\n",
    "                cv2.putText(frame, f'{classes_names[int(box.cls[0])]} {box.conf[0]:.2f}', (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1, colour, 2)\n",
    "                \n",
    "    # show the image\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    # break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the video capture and destroy all windows\n",
    "videoCap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
